{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KameronKeller/Grid-World-Deep-Q-Learning/blob/main/GridWorldDeepQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SoVnzS8UHDs"
      },
      "source": [
        "# Grid World Deep Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu2HDJf6UObF"
      },
      "source": [
        "***\n",
        "# Load packages \n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o5aZVdWtmC-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a8b605-d6c2-4f06-e849-154e59407c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.27.0-py3-none-any.whl (879 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m879.1/879.1 KB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (4.4.0)\n",
            "Collecting gymnasium-notices>=0.0.1\n",
            "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
            "Collecting jax-jumpy>=0.2.0\n",
            "  Downloading jax_jumpy-0.2.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (6.0.0)\n",
            "Collecting shimmy<1.0,>=0.1.0\n",
            "  Downloading Shimmy-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium) (2.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (3.11.0)\n",
            "Installing collected packages: gymnasium-notices, jax-jumpy, shimmy, gymnasium\n",
            "Successfully installed gymnasium-0.27.0 gymnasium-notices-0.0.1 jax-jumpy-0.2.0 shimmy-0.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium[toy_text] in /usr/local/lib/python3.8/dist-packages (0.27.0)\n",
            "Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (0.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (6.0.0)\n",
            "Requirement already satisfied: jax-jumpy>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (2.2.0)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (0.2.0)\n",
            "Collecting pygame==2.1.3.dev8\n",
            "  Downloading pygame-2.1.3.dev8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium[toy_text]) (3.11.0)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.3.dev8\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install gymnasium[toy_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yk4gw-zCxuBa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OWR704wImUbE"
      },
      "outputs": [],
      "source": [
        "# Setup plotting to display matplotlib animations\n",
        "plt.rc('animation', html='jshtml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSn4CcZb7LyW"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4rG1uwiKUSvS",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# load your dataset \n",
        "# Create a 'FrozenLake' environment. Initally, is_slippery is false to prevent agent from unpredictable sliding on the lake\n",
        "# More info: https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WwbaWMhJjdH"
      },
      "source": [
        "### Pre-process dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdahaYaCJl3a",
        "outputId": "ff7309bd-b638-4eb2-915b-840d0db8ebc8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of states:  16\n",
            "Number of actions:  4\n"
          ]
        }
      ],
      "source": [
        "# Number of states\n",
        "n_states = env.observation_space.n\n",
        "print(\"Number of states: \", n_states)\n",
        "\n",
        "# Determine the number of outputs\n",
        "n_actions = env.action_space.n\n",
        "print(\"Number of actions: \", n_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPs0aPJy0Gk6"
      },
      "source": [
        "***\n",
        "# Workspace\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGb3fppfwVle"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ju1oLG_Qusyc"
      },
      "outputs": [],
      "source": [
        "# Use the model to predict the next move\n",
        "def next_move(obs, model):\n",
        "\n",
        "    # Get the predictions for possible actions\n",
        "    moves = model.predict([obs], verbose=0)[0]\n",
        "\n",
        "    # Return the best prediction \n",
        "    action = np.argmax(moves)\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TLmQzwrsvCfP"
      },
      "outputs": [],
      "source": [
        "# Play the game\n",
        "# Adapted from: Hands on Machine Learning by G√©ron\n",
        "def play_game(env, model):\n",
        "    n_max_steps = 100\n",
        "    frames = []\n",
        "    obs, info = env.reset()\n",
        "    for step in range(n_max_steps):\n",
        "        frames.append(env.render())\n",
        "        action = next_move(obs, model)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "    env.close()\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dktSJayTey5z"
      },
      "outputs": [],
      "source": [
        "# Get sample experiences from the replay buffer\n",
        "# Source: Hands on Machine Learning by G√©ron\n",
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
        "    batch = [replay_buffer[index] for index in indices]\n",
        "    return [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(6)\n",
        "    ]  # [states, actions, rewards, next_states, dones, truncateds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6pti4pP-hXpf"
      },
      "outputs": [],
      "source": [
        "# The training step\n",
        "# Source: Hands on Machine Learning by G√©ron\n",
        "def training_step(batch_size, optimizer, discount_factor):\n",
        "\n",
        "    # Get a sample batch of experiences\n",
        "    experiences = sample_experiences(batch_size)\n",
        "\n",
        "    # Get the variables of those experiences\n",
        "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
        "\n",
        "    # Get the Q-Value for the each possible action\n",
        "    next_Q_values = model.predict(next_states, verbose=0)\n",
        "\n",
        "    # Get the max Q-Values from the predictions\n",
        "    max_next_Q_values = next_Q_values.max(axis=1)\n",
        "\n",
        "    # Runs = 1 if episode runs\n",
        "    runs = 1.0 - (dones | truncateds)  # episode is not done or truncated\n",
        "\n",
        "    # Compute target Q value\n",
        "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
        "\n",
        "    # Reshape the output to appropriate format\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "\n",
        "    # Mask all actions we do not need. Example: mask = [[0 0 0 1], [1 0 0 0]...]\n",
        "    mask = tf.one_hot(actions, n_actions)\n",
        "\n",
        "    # Gradient descent step to minimize the loss\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Return loss for plotting\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "C5bGhGn2w4oL"
      },
      "outputs": [],
      "source": [
        "# Return a random action based on probability, or choose the best action\n",
        "# Source: Hands on Machine Learning by G√©ron\n",
        "def epsilon_greedy_policy(state, epsilon, n_actions):\n",
        "    if np.random.rand() < epsilon: # original\n",
        "        return np.random.randint(n_actions)  # random action\n",
        "    else:\n",
        "        Q_values = model.predict([state], verbose=0)\n",
        "        return Q_values.argmax()  # optimal action according to the DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8-hPhBTke7jl"
      },
      "outputs": [],
      "source": [
        "# Play one step in the environment\n",
        "# Source: Hands on Machine Learning by G√©ron\n",
        "def play_one_step(env, state, epsilon, n_actions):\n",
        "    action = epsilon_greedy_policy(state, epsilon, n_actions)\n",
        "    next_state, reward, done, truncated, info = env.step(action)\n",
        "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
        "    return next_state, reward, done, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "t8DnkWDG6oqr"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "# Adapted from: Hands on Machine Learning by G√©ron\n",
        "def train_model(env, model, num_iterations, optimizer, discount_factor, verbose=True):\n",
        "  # Store the number of wins (1 for each time 'done' is true)\n",
        "  win_counter = 0\n",
        "\n",
        "  # Store the rewards from each episode for graphing\n",
        "  rewards = []\n",
        "\n",
        "  # Store the losses for graphing\n",
        "  losses = []\n",
        "\n",
        "  # Variable for storing the best score\n",
        "  best_score = 0\n",
        "\n",
        "  for episode in range(num_iterations):\n",
        "\n",
        "      # Reset the environment\n",
        "      obs, info = env.reset()\n",
        "\n",
        "      # 100 steps is the max for the frozen lake environment\n",
        "      for step in range(100): # max for frozen lake\n",
        "\n",
        "          # Epsilon is the probability of exploring. It decays to 0.01 after the world is explored\n",
        "          epsilon = max(1 - episode / 500, 0.01) # probability of exploring\n",
        "\n",
        "          # Play one step and get the variables\n",
        "          obs, reward, done, truncated, info = play_one_step(env, obs, epsilon, n_actions)\n",
        "\n",
        "          # Keep track of the wins\n",
        "          if reward == 1:\n",
        "            win_counter += 1\n",
        "\n",
        "          # if done or truncated, start next episode\n",
        "          if done or truncated:\n",
        "            break\n",
        "\n",
        "      # extra code ‚Äì displays debug info, stores data for the next figure, and\n",
        "      #              keeps track of the best model weights so far\n",
        "      if verbose:\n",
        "        print(f\"\\rEpisode: {episode + 1}, Steps: {step + 1}, eps: {epsilon:.3f}\",\n",
        "              end=\"\")\n",
        "\n",
        "      # Append the reward to rewards for graphing    \n",
        "      rewards.append(reward)\n",
        "    \n",
        "      # The best model will obtain the reward in the least moves\n",
        "      if reward >= best_score: \n",
        "          best_weights = model.get_weights()\n",
        "          best_score = reward\n",
        "\n",
        "      # Wait till replay buffer is filled before training\n",
        "      if episode > 50:\n",
        "          loss = training_step(batch_size, optimizer, discount_factor)\n",
        "          losses.append(loss)\n",
        "\n",
        "  # Calculate the win ratio\n",
        "  win_ratio = win_counter / num_iterations\n",
        "\n",
        "  # Restore the best model weights\n",
        "  model.set_weights(best_weights)\n",
        "\n",
        "  return model, rewards, losses, win_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eWzjUq9Z4N85"
      },
      "outputs": [],
      "source": [
        "# Plot the sum of rewards\n",
        "# Source: Hands on Machine Learning by G√©ron  \n",
        "def plot_rewards(rewards):\n",
        "  plt.figure(figsize=(8, 4))\n",
        "  plt.plot(rewards)\n",
        "  plt.title(\"Rewards\")\n",
        "  plt.xlabel(\"Episode\", fontsize=14)\n",
        "  plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "X5i6T7CqBwql"
      },
      "outputs": [],
      "source": [
        "# Plot the losses\n",
        "def plot_losses(losses):\n",
        "  plt.figure(figsize=(8, 4))\n",
        "  plt.title(\"Losses per Episode\")\n",
        "  plt.xlabel(\"Episode\", fontsize=14)\n",
        "  plt.ylabel(\"Losses\", fontsize=14)\n",
        "  plt.plot(losses)\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zSiquLS_enDT"
      },
      "outputs": [],
      "source": [
        "# Plot the win ratios\n",
        "def plot_win_ratios(hyperparameter, win_ratios):\n",
        "  plt.figure(figsize=(8, 4))\n",
        "  plt.title(\"Win ratios per hyperparameter\")\n",
        "  plt.xlabel(\"Hyperparameter Value\", fontsize=14)\n",
        "  plt.ylabel(\"Win Ratio\", fontsize=14)\n",
        "  plt.plot(hyperparameter, win_ratios)\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JhsbGf-ewTEM"
      },
      "outputs": [],
      "source": [
        "# Plot the an environment\n",
        "# Source: Hands on Machine Learning by G√©ron\n",
        "def plot_environment(env, figsize=(5, 4)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    img = env.render()\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "91_vpceGXtRB"
      },
      "outputs": [],
      "source": [
        "# Display an animation of an episode\n",
        "# Source: Hands on Machine Learning by G√©ron\n",
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "i9CVyJG7yRYO"
      },
      "outputs": [],
      "source": [
        "# Plot the animation\n",
        "# Source: Hands on Machine Learning by G√©ron\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = matplotlib.animation.FuncAnimation(\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrfQ-1Tozk2T"
      },
      "source": [
        "##### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oxXHKs66618I"
      },
      "outputs": [],
      "source": [
        "# Build a neural network model\n",
        "def build_model(n_states, n_actions, dense_units, embedding_output_dim):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(n_states, embedding_output_dim),\n",
        "    tf.keras.layers.Dense(dense_units, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(n_actions)\n",
        "  ])\n",
        "  # model.summary()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG5NCz1b0Jie"
      },
      "source": [
        "## Experiment(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldeYOmuWa37d"
      },
      "source": [
        "##### Experiment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo9uSovyXVNY"
      },
      "source": [
        "##### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6p5OoeE4a3Wm"
      },
      "outputs": [],
      "source": [
        "# Set up deque as a replay buffer\n",
        "replay_buffer = deque(maxlen=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2U-cOUIUfFHk"
      },
      "outputs": [],
      "source": [
        "# Initial hyperparameters\n",
        "# Number of iterations\n",
        "iterations = 500\n",
        "\n",
        "# Dense layer units\n",
        "dense_units = 32\n",
        "\n",
        "# Embedding output dimensions\n",
        "embedding_output_dim = 4\n",
        "\n",
        "# The batch size to sample\n",
        "batch_size = 64\n",
        "\n",
        "# The discount factor\n",
        "discount_factor = 0.95\n",
        "\n",
        "# Learning Rate\n",
        "learning_rate = .001\n",
        "\n",
        "# The optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# The loss function\n",
        "loss_fn = tf.keras.losses.mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9FGobpk6afs0"
      },
      "outputs": [],
      "source": [
        "# Display the results of training\n",
        "def display_results(results):\n",
        "  model, rewards, losses, win_ratio = results\n",
        "  print(\"Win ratio: {:.2f}\".format(win_ratio))\n",
        "  plot_rewards(rewards)\n",
        "  plot_losses(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Baseline Experiment"
      ],
      "metadata": {
        "id": "5CD2Y7PpaZrw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "wwkTe7YUbcaJ"
      },
      "outputs": [],
      "source": [
        "model = build_model(n_states, n_actions, dense_units, embedding_output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13aj1JBQ7YUs",
        "outputId": "cf049840-334d-458a-f71c-88bd513a1d16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None,).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 375, Steps: 15, eps: 0.252"
          ]
        }
      ],
      "source": [
        "results = train_model(env, model, 1000, optimizer, discount_factor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfa8A0QrYnZa"
      },
      "outputs": [],
      "source": [
        "display_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZwVn6QoZsuN"
      },
      "source": [
        "#### Experiment 1 - Number of Dense Units"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgJrdMGQZsgw"
      },
      "outputs": [],
      "source": [
        "test_dense_units = [4, 8, 16, 32, 64]\n",
        "win_ratios = []\n",
        "\n",
        "for count, units in enumerate(test_dense_units):\n",
        "  print(\"========== Experiment: Dense Units = {} ==========\".format(units))\n",
        "  model = build_model(n_states, n_actions, units, embedding_output_dim)\n",
        "  results = train_model(env, model, iterations, optimizer, discount_factor, verbose=False)\n",
        "  win_ratio = results[3]\n",
        "  win_ratios.append(win_ratio)\n",
        "  display_results(results)\n",
        "\n",
        "print(\"========== Results ==========\")\n",
        "best_parameter_index = np.argmax(win_ratios)\n",
        "print(\"Best Parameter = {}, with win ratio of {:.2f}\".format(test_dense_units[best_parameter_index], win_ratios[best_parameter_index]))\n",
        "plot_win_ratios(test_dense_units, win_ratios)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpuPR3hiKw-"
      },
      "source": [
        "#### Experiment 2 - Number of Embedding Layer Output Dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLaWkEZoiTLQ"
      },
      "outputs": [],
      "source": [
        "test_embedding_output_dims = [2, 3, 4, 5, 6]\n",
        "win_ratios = []\n",
        "\n",
        "for count, output_dim in enumerate(test_embedding_output_dims):\n",
        "  print(\"========== Experiment: Embedding Layer Output Dim = {} ==========\".format(output_dim))\n",
        "  model = build_model(n_states, n_actions, dense_units, output_dim)\n",
        "  results = train_model(env, model, iterations, optimizer, discount_factor, verbose=False)\n",
        "  win_ratio = results[3]\n",
        "  win_ratios.append(win_ratio)\n",
        "  display_results(results)\n",
        "\n",
        "print(\"========== Results ==========\")\n",
        "best_parameter_index = np.argmax(win_ratios)\n",
        "print(\"Best Parameter = {}, with win ratio of {:.2f}\".format(test_embedding_output_dims[best_parameter_index], win_ratios[best_parameter_index]))\n",
        "plot_win_ratios(test_embedding_output_dims, win_ratios)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeU0C10XjTXC"
      },
      "source": [
        "#### Experiment 3 - Discount Factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJUgqzXsjTrm"
      },
      "outputs": [],
      "source": [
        "test_discount_factors = [0.92, 0.95, 0.97, 0.99]\n",
        "win_ratios = []\n",
        "\n",
        "for count, discount_factor in enumerate(test_discount_factors):\n",
        "  print(\"========== Experiment: Discount Factor = {} ==========\".format(discount_factor))\n",
        "  model = build_model(n_states, n_actions, dense_units, embedding_output_dim)\n",
        "  results = train_model(env, model, iterations, optimizer, discount_factor, verbose=False)\n",
        "  win_ratio = results[3]\n",
        "  win_ratios.append(win_ratio)\n",
        "  display_results(results)\n",
        "\n",
        "print(\"========== Results ==========\")\n",
        "best_parameter_index = np.argmax(win_ratios)\n",
        "print(\"Best Parameter = {}, with win ratio of {:.2f}\".format(test_discount_factors[best_parameter_index], win_ratios[best_parameter_index]))\n",
        "plot_win_ratios(test_discount_factors, win_ratios)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDgXT6W9lhW7"
      },
      "source": [
        "#### Experiment 4 - Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCE2-6BVljs6"
      },
      "outputs": [],
      "source": [
        "adam = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "sgd = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Reset the discount_factor\n",
        "discount_factor = 0.95\n",
        "test_optimizers = [adam, sgd]\n",
        "win_ratios = []\n",
        "\n",
        "for count, optimizer in enumerate(test_optimizers):\n",
        "  print(\"========== Experiment: Optimizer = {} ==========\".format(optimizer))\n",
        "  model = build_model(n_states, n_actions, dense_units, embedding_output_dim)\n",
        "  results = train_model(env, model, iterations, optimizer, discount_factor, verbose=False)\n",
        "  win_ratio = results[3]\n",
        "  win_ratios.append(win_ratio)\n",
        "  display_results(results)\n",
        "\n",
        "print(\"========== Results ==========\")\n",
        "best_parameter_index = np.argmax(win_ratios)\n",
        "print(\"Best Parameter = {}, with win ratio of {:.2f}\".format(test_optimizers[best_parameter_index], win_ratios[best_parameter_index]))\n",
        "plot_win_ratios(list(range(len(test_optimizers))), win_ratios)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZSpO2-KmTkg"
      },
      "source": [
        "#### Experiment 5 - Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lJslfW8mTCM"
      },
      "outputs": [],
      "source": [
        "test_learning_rates = [0.001, 0.0001, 0.00001]\n",
        "win_ratios = []\n",
        "\n",
        "for count, learning_rate in enumerate(test_learning_rates):\n",
        "  print(\"========== Experiment: Learning Rate = {} ==========\".format(learning_rate))\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  model = build_model(n_states, n_actions, dense_units, embedding_output_dim)\n",
        "  results = train_model(env, model, iterations, optimizer, discount_factor, verbose=False)\n",
        "  win_ratio = results[3]\n",
        "  win_ratios.append(win_ratio)\n",
        "  display_results(results)\n",
        "\n",
        "print(\"========== Results ==========\")\n",
        "best_parameter_index = np.argmax(win_ratios)\n",
        "print(\"Best Parameter = {}, with win ratio of {:.2f}\".format(test_learning_rates[best_parameter_index], win_ratios[best_parameter_index]))\n",
        "plot_win_ratios(test_learning_rates, win_ratios)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyT9tz_xYnZc"
      },
      "outputs": [],
      "source": [
        "# Build a new model with the best hyperparameters\n",
        "\n",
        "# Number of iterations\n",
        "iterations = 1000\n",
        "\n",
        "# Dense layer units\n",
        "dense_units = 32\n",
        "\n",
        "# Embedding output dimensions\n",
        "embedding_output_dim = 5\n",
        "\n",
        "# The batch size to sample\n",
        "batch_size = 64\n",
        "\n",
        "# The discount factor\n",
        "discount_factor = 0.92\n",
        "\n",
        "# Learning Rate\n",
        "learning_rate = .001\n",
        "\n",
        "# The optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# The loss function\n",
        "loss_fn = tf.keras.losses.mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6pRmxTeYnZd"
      },
      "outputs": [],
      "source": [
        "model = build_model(n_states, n_actions, dense_units, embedding_output_dim)\n",
        "results = train_model(env, model, iterations, optimizer, discount_factor, verbose=True)\n",
        "display_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svP2m5j-x0Z7"
      },
      "source": [
        "***\n",
        "# Report\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn9l0JqG-Iho"
      },
      "source": [
        "## Deep Q-Learning with Frozen Lake"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "LQ7rhONcbefV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_environment(env)"
      ],
      "metadata": {
        "id": "QCUMOjUmbiXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU1LL5MTYnZe"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "[Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) is a simple grid world where an agent must dodge holes in a frozen lake in order to reach a reward. A combination of Q-Learning and Deep Learning, Deep Q-Learning (DQN), was used to successfully train an agent to navigate the grid world to reach a reward. \n",
        " \n",
        "To determine the optimal hyperparameters for the machine learning model, research was conducted on the impacts of five different hyperparameters on the model's effectiveness. The most effective hyperparameters were then chosen and applied to a new model. Finally, the new model was tested to determine a final outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyJ4hqh1YnZe"
      },
      "source": [
        "### Dataset and Pre-processing\n",
        "In contrast to a traditional machine learning problem where a large dataset is analyzed and used to make predictions, DQNs utilize an actor-critic method for generating training data. The actor-critic method involves an agent acting on an environment. Based on the agent's action, either a reward or punishment is received. This outcome is fed into the DQN and used to make future predictions based on a given state in the environment.\n",
        "\n",
        "In the Frozen Lake environment, there are 16 possible states. The agent begins in the top left corner of the map, and a gift is in the bottom right corner of the map. The agent has four possible actions: move up, down, left, or right. If the agent reaches the gift, it receives a reward of \"1\", and for all other spaces it receives a reward of \"0\". \n",
        "\n",
        "<img src=\"https://gymnasium.farama.org/_images/AE_loop.png\" alt=\"actor-critic\" width=\"400\"/>\n",
        "\n",
        "[Source](https://gymnasium.farama.org/content/basic_usage/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRgjfzKgYnZe"
      },
      "source": [
        "### Methods\n",
        "The process for building a DQN and optimizing it followed these basic steps:\n",
        "1. Build a working DQN, including helper functions needed to output Q-values (quality values).\n",
        "2. Identify key hyperparameters and run tests on these parameters to determine their effect on the model.\n",
        "3. Build and test a new model based on the highest-performing hyperparameters from step 2.\n",
        "\n",
        "#### Building the DQN \n",
        "To build the DQN, several components are needed. It needs a neural network, a replay buffer, a greedy function, and a training function. \n",
        "\n",
        "The neural network for this problem was composed of the following layers:\n",
        "* An embedding layer with 16 input dimensions (to match the 4x4 grid)\n",
        "* A dense layer\n",
        "* An output layer, with 4 outputs (to match the 4 possible actions the agent may take)\n",
        "\n",
        "An embedding layer is useful for this problem, as it maps indices to dense vectors. It is an efficient way to encode inputs as one-hot vectors.\n",
        "\n",
        "This implementation uses a replay buffer (deque) to store the agent's experiences during training. For each training episode, the replay buffer is sampled at random so that the DQN training is not based only on the most recent experiences. This improves the quality of the training by reducing correlations in the training batch. \n",
        "\n",
        "When a new Frozen Lake environment is created, the untrained model is blind to the results of movement to any new space on the map. Therefore, it must incorporate a sort of exploration policy so that it may explore the map. Without this exploration, the agent may never find the reward and will get stuck as no moves provide any benefit. To accomplish this, an `epsilon greedy policy` is used. Epsilon is the probability that the agent will explore. In this implementation, during training, the value of epsilon is linearly decreased until a minimum value (0.01). In other words, after each episode, the agent has a lower chance of exploring new spaces. In theory, by the time epsilon reaches a minimum, the agent has located the reward and has positive reinforcement to visit that area again. When the agent does not explore new spaces, it makes a greedy choice based on predicted values, given its state. That is, at any given location on the map, the agent chooses the one with the highest hope of reward.\n",
        "\n",
        "With these components in place the following training algorithm was utilized (G√©ron, p. 636):\n",
        "1. Get a sample batch of experiences from the replay buffer.\n",
        "2. Predict the Q-values based on the state the sample experience left the agent in.\n",
        "3. Choose the best Q-values.\n",
        "4. Calculate the target Q-values. The target Q-value is the result of this equation: ùëÑ<sub>target</sub>(ùë†, ùëé) = ùëü + ùõæ max ùëÑ(ùë†‚Ä≤,ùëé‚Ä≤), where `r` is the rewards, `ùõæ` is the discount factor, and `max ùëÑ(ùë†‚Ä≤,ùëé‚Ä≤)` is the max Q-value from step 3.\n",
        "5. One-hot encode the actions to zero-out unwanted Q-values.\n",
        "6. Compute the loss and perform a gradient descent step to minimize the loss.\n",
        "\n",
        "The discount factor in step 4 is critical. When the agent reaches the reward, every step leading up to the reward is given a fraction of the reward. The further the location from the final reward, the smaller the reward received.\n",
        "\n",
        "#### Identifying the Best Hyperparameters\n",
        "Through trial and error, five hyperparameters were identified that had a strong influence on the model's effectiveness. \n",
        "1. The number of units in the dense layer.\n",
        "2. The output dimensions of the embedding layer.\n",
        "3. The discount factor.\n",
        "4. The learning rate.\n",
        "5. The optimizer.\n",
        "\n",
        "Initially, the following baseline parameters were chosen:\n",
        "```python\n",
        "# Dense layer units\n",
        "dense_units = 32\n",
        "\n",
        "# Embedding output dimensions\n",
        "embedding_output_dim = 4\n",
        "\n",
        "# Discount actor\n",
        "discount_factor = 0.95\n",
        "\n",
        "# Learning Rate\n",
        "learning_rate = .001\n",
        "\n",
        "# The optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "```\n",
        "\n",
        "The following values were tested in small batches of 300 iterations:\n",
        "\n",
        "```python\n",
        "# Dense layer units\n",
        "dense_units = [4, 8, 16, 32, 64]\n",
        "\n",
        "# Embedding output dimensions\n",
        "embedding_output_dim = [2, 3, 4, 5, 6]\n",
        "\n",
        "# Discount actor\n",
        "discount_factor = [0.92, 0.95, 0.97, 0.99]\n",
        "\n",
        "# Learning Rate\n",
        "learning_rate = .001\n",
        "\n",
        "# The optimizer\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "sgd = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
        "\n",
        "optimizer = [adam, sgd]\n",
        "```\n",
        "\n",
        "Each the exception of the hyperparameter being tested, all other hyperparameters were set to the baseline parameters. The primary metric used to test the impact of the hyperparameters was a ratio of wins (reward reached) to the number of iterations (episodes).\n",
        "\n",
        "Finally, the best hyperparameters were chosen, a new model was created, tested on 1,000 iterations, and compared to the baseline test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN2e3LKOYnZe"
      },
      "source": [
        "### Results and Analysis\n",
        "By independently analyzing the five hyperparameters specified above and tuning the model to the best hyperparameters, a 11% increase in the win ratio was achieved. The baseline test produced a win ratio of 57%, while the final test produced a win ratio of 68. The final model was not only successful but efficient. After approximately 175 iterations, the model often reached the \"gift\" in the environment.\n",
        "\n",
        "| Model          | Win Ratio |\n",
        "|----------------|-----------|\n",
        "| Baseline Model | 0.57      |\n",
        "| Updated Model  | 0.68      |\n",
        "\n",
        "The below tables summarize the impact of each hyperparameter on the win ratio. The optimizer and learning rates had the largest impact on the win ratio. Discount factors below 0.99 appeared to be necessary for an effective model.\n",
        "\n",
        "#### Number of Dense Units\n",
        "\n",
        "| Number of Dense   Units | Win Ratio |\n",
        "|-------------------------|-----------|\n",
        "| 4                       | 0.01      |\n",
        "| 8                       | 0.38      |\n",
        "| 16                      | 0.36      |\n",
        "| 32                      | 0.40      |\n",
        "| 64                      | 0.29      |\n",
        "\n",
        "#### Number of Embedding Layer Output Dimensions\n",
        "| Number of Embedding   Layer Output Dimensions | Win Ratio |\n",
        "|-----------------------------------------------|-----------|\n",
        "| 2                                             | 0.28      |\n",
        "| 3                                             | 0.20      |\n",
        "| 4                                             | 0.35      |\n",
        "| 5                                             | 0.42      |\n",
        "| 6                                             | 0.37      |\n",
        "\n",
        "#### Discount Factor\n",
        "| Discount Factor | Win Ratio |\n",
        "|-----------------|-----------|\n",
        "| 0.92            | 0.40      |\n",
        "| 0.95            | 0.37      |\n",
        "| 0.97            | 0.09      |\n",
        "| 0.99            | 0.00      |\n",
        "\n",
        "#### Optimizer\n",
        "| Optimizer | Win Ratio |\n",
        "|-----------|-----------|\n",
        "| Adam      | 0.33      |\n",
        "| SGD       | 0.00      |\n",
        "\n",
        "#### Learning Rate\n",
        "| Learning Rate | Win Ratio |\n",
        "|---------------|-----------|\n",
        "| 0.001         | 0.34      |\n",
        "| 0.0001        | 0.07      |\n",
        "| 0.00001       | 0.01      |\n",
        "\n",
        "#### Successfully Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frames = play_game(env, model)\n",
        "plot_animation(frames)"
      ],
      "metadata": {
        "id": "79MJyxJSavNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGGnOqyPYnZe"
      },
      "source": [
        "### Discussion and Conclusion\n",
        "DQN was a successful deep-learning model for solving Frozen Lake. Adjusting the hyperparameters produced a fast learning model with an overall win ratio of 69% during training.  Of the hyperparameters tested, the most important were the learning rate and the optimizer. A learning rate of 0.001 produced a 27% improvement over 0.0001. However, this is not a direct improvement. Had the model trained on more iterations, the slower learning rate might have produced better results. This is because a slow learning rate takes many iterations to find the optimal solution, while a faster learning rate may find it quickly but is possibly less accurate.\n",
        "\n",
        "The Adam optimizer substantially outperformed the SGD optimizer. This is likely because it stores adaptive learning rates and requires less tuning than SGD. In this experiment, the parameters of the SGD optimizer were not tested or fine-tuned. It is possible that further testing will result in similar performance between Adam and SGD.\n",
        "\n",
        "This experiment involved manually picking hyperparameters and testing them individually, largely due to the computation time needed for testing. Future testing utilizing methods like grid search would allow for a more expansive look into how each hyperparameter affects the other. Additionally, testing each hyperparameter on larger datasets (n > 10,000) may reveal new patterns that were hidden by this experiment's small test sizes. Finally, the range of values for each hyperparameter tested was narrow in this test. Expanding these ranges may also further optimizations and additional patterns not seen here."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "\n",
        "\n",
        "*   Google: [Playing CartPole with the Actor-Critic method](https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic#train_the_agent)\n",
        "*   G√©ron, A. *Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow*\n",
        "* Jordan, J. [Setting the learning rate of your neural network.](https://www.jeremyjordan.me/nn-learning-rate/)\n",
        "* Causevic, S. [Deep Reinforcement Learning: Build a Deep Q-network(DQN) with TensorFlow 2 and Gym to Play CartPole](https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998)\n",
        "* Stack Overflow: (What is the difference between an Embedding Layer and a Dense Layer?)(https://stackoverflow.com/questions/47868265/what-is-the-difference-between-an-embedding-layer-and-a-dense-layer)\n",
        "\n"
      ],
      "metadata": {
        "id": "FHvCELLkckdu"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iJ35u3SZ5_BO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}